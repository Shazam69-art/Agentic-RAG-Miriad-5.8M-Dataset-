
import gradio as gr
import google.generativeai as genai
import faiss
import joblib
import numpy as np
from sentence_transformers import SentenceTransformer
import time
from typing import List, Dict, Tuple
import os

# ==================== CONFIGURATION ====================
GEMINI_API_KEY = "" 

# File paths
FAISS_INDICES = {
    'rag_3': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/ivfpq_optimized/rag_3_ivfpq_index.bin',
    'rag_4': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/ivfpq_optimized/rag_4_ivfpq_index.bin',
    'rag_5': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/ivfpq_optimized/rag_5_ivfpq_index.bin',
    'rag_6': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/ivfpq_optimized/rag_6_ivfpq_index.bin'
}

JOBLIB_FILES = {
    'rag_3': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/rag_3_qa_data.joblib',
    'rag_4': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/rag_4_qa_data.joblib',
    'rag_5': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/rag_5_qa_data.joblib',
    'rag_6': '/Volumes/CONFIDENTIAL/Aetherion/RAG/Optimized Final RAG/rag_6_qa_data.joblib'
}


# ==================== AGENTIC RAG SYSTEM ====================
class AgenticRAGSystem:
    """
    Advanced Agentic RAG Architecture with:
    - Multi-step reasoning
    - Query decomposition
    - Retrieval verification
    - Self-reflection and refinement
    """

    def __init__(self, api_key: str):
        print("ğŸš€ Initializing Agentic RAG System...")

        # Configure Gemini
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.5-flash-lite')

        # Load embedding model
        print("ğŸ“Š Loading embedding model...")
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

        # Load FAISS indices and data
        self.indices = {}
        self.data = {}

        print("ğŸ” Loading FAISS indices and knowledge bases...")
        for key in FAISS_INDICES:
            try:
                # Load FAISS index
                self.indices[key] = faiss.read_index(FAISS_INDICES[key])
                print(f"  âœ“ Loaded {key} index: {self.indices[key].ntotal} vectors")

                # Load corresponding data
                self.data[key] = joblib.load(JOBLIB_FILES[key])
                print(f"  âœ“ Loaded {key} data: {len(self.data[key])} entries")
            except Exception as e:
                print(f"  âœ— Error loading {key}: {e}")

        print("âœ… System initialization complete!\n")

    def _decompose_query(self, query: str) -> List[str]:
        """Agent Step 1: Decompose complex queries into sub-queries"""
        prompt = f"""You are a query decomposition agent. Break down this medical query into 2-3 focused sub-queries.

Original Query: {query}

Provide sub-queries as a numbered list. Each sub-query should target a specific aspect.
If the query is already simple, return just the original query."""

        try:
            response = self.model.generate_content(prompt)
            sub_queries = [query]  # Always include original

            if response.text:
                lines = response.text.strip().split('\n')
                for line in lines:
                    if line.strip() and any(c.isalnum() for c in line):
                        clean = line.strip().lstrip('0123456789.-) ')
                        if clean and clean != query:
                            sub_queries.append(clean)

            return sub_queries[:3]  # Limit to 3 queries
        except:
            return [query]

    def _retrieve_context(self, query: str, top_k: int = 5) -> List[Dict]:
        """Agent Step 2: Retrieve relevant context from all knowledge bases"""
        # Embed query
        query_embedding = self.embedder.encode([query])[0].astype('float32')
        query_embedding = np.expand_dims(query_embedding, axis=0)

        all_results = []

        # Search across all indices
        for key, index in self.indices.items():
            try:
                distances, indices = index.search(query_embedding, top_k)

                for dist, idx in zip(distances[0], indices[0]):
                    if idx < len(self.data[key]):
                        data_item = self.data[key][idx]

                        # Handle different data formats
                        if isinstance(data_item, dict):
                            result = data_item.copy()
                        elif isinstance(data_item, (tuple, list)):
                            # Assume format is (question, answer) or similar
                            result = {
                                'question': str(data_item[0]) if len(data_item) > 0 else '',
                                'answer': str(data_item[1]) if len(data_item) > 1 else ''
                            }
                        else:
                            result = {'content': str(data_item)}

                        result['source'] = key
                        result['relevance_score'] = float(1 / (1 + dist))
                        all_results.append(result)
            except Exception as e:
                print(f"Search error in {key}: {e}")

        # Sort by relevance and return top results
        all_results.sort(key=lambda x: x['relevance_score'], reverse=True)
        return all_results[:top_k * 2]

    def _verify_retrieval(self, query: str, contexts: List[Dict]) -> List[Dict]:
        """Agent Step 3: Verify and filter retrieved contexts"""
        if not contexts:
            return []

        # Filter by relevance threshold
        threshold = 0.3
        verified = [c for c in contexts if c.get('relevance_score', 0) > threshold]

        return verified[:10]  # Return top 10

    def _generate_with_reflection(self, query: str, contexts: List[Dict]) -> str:
        """Agent Step 4: Generate answer with self-reflection"""

        # Format context
        context_text = "\n\n".join([
            f"[Source: {c['source']} | Relevance: {c['relevance_score']:.2f}]\n{c.get('question', '')}\n{c.get('answer', '')}"
            for c in contexts[:5]
        ])

        # Generate initial response
        prompt = f"""You are an expert medical AI assistant with access to verified medical knowledge.

RETRIEVED CONTEXT:
{context_text}

USER QUERY: {query}

INSTRUCTIONS:
1. Analyze the retrieved context carefully
2. Provide accurate, evidence-based medical information
3. If context is insufficient, acknowledge limitations
4. Use professional medical terminology appropriately
5. Be empathetic and clear

Generate a comprehensive response:"""

        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"I apologize, but I encountered an error: {str(e)}"

    def process_query(self, query: str, history: List = None) -> Tuple[str, List[Dict]]:
        """Main agentic RAG pipeline"""

        # Step 1: Query Decomposition
        sub_queries = self._decompose_query(query)

        # Step 2: Retrieve context for all sub-queries
        all_contexts = []
        for sq in sub_queries:
            contexts = self._retrieve_context(sq)
            all_contexts.extend(contexts)

        # Remove duplicates
        seen = set()
        unique_contexts = []
        for c in all_contexts:
            key = (c.get('question', ''), c.get('source', ''))
            if key not in seen:
                seen.add(key)
                unique_contexts.append(c)

        # Step 3: Verify and filter
        verified_contexts = self._verify_retrieval(query, unique_contexts)

        # Step 4: Generate with reflection
        response = self._generate_with_reflection(query, verified_contexts)

        return response, verified_contexts


# ==================== GRADIO INTERFACE ====================
def create_interface():
    """Create professional ChatGPT-style interface"""

    # Initialize RAG system
    rag_system = AgenticRAGSystem(GEMINI_API_KEY)

    def respond(message, history):
        """Process user message with streaming effect"""
        if not message.strip():
            return history

        # Initialize history if None
        if history is None:
            history = []

        # Add user message to history (new Gradio format)
        history.append({"role": "user", "content": message})

        # Generate response
        response, contexts = rag_system.process_query(message, history)

        # Add assistant message placeholder
        history.append({"role": "assistant", "content": ""})

        # Stream response with typing effect
        partial_response = ""
        for i in range(0, len(response), 3):
            partial_response = response[:i + 3]
            history[-1]["content"] = partial_response
            time.sleep(0.01)
            yield history

        # Final response
        history[-1]["content"] = response
        yield history

    # Create interface
    with gr.Blocks(title="Medical RAG Assistant") as demo:
        gr.Markdown(
            """
            # ğŸ¥ Medical RAG Assistant
            ### Powered by Gemini AI & Agentic RAG Architecture

            Ask any medical question and get evidence-based answers from our comprehensive knowledge base.
            """
        )

        chatbot = gr.Chatbot(
            height=500
        )

        with gr.Row():
            msg = gr.Textbox(
                placeholder="Ask a medical question...",
                show_label=False,
                lines=1
            )
            submit = gr.Button("Send ğŸš€", variant="primary")

        gr.Markdown(
            """
            ---
            **Note:** This system uses advanced Agentic RAG with query decomposition, multi-source retrieval, 
            and self-reflection for accurate medical information.
            """
        )

        # Event handlers
        msg.submit(respond, [msg, chatbot], [chatbot])
        submit.click(respond, [msg, chatbot], [chatbot])
        msg.submit(lambda: "", None, [msg])
        submit.click(lambda: "", None, [msg])

    return demo


# ==================== MAIN ====================
if __name__ == "__main__":
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     Medical RAG Assistant - Agentic Architecture          â•‘
    â•‘     Powered by Gemini AI & FAISS Vector Search            â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Create and launch interface
    demo = create_interface()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        show_error=True
    )
